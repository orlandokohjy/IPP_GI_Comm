{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is the final script for GI Commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard import\n",
    "from datetime import date\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "folder_path = input(\"Please input the directory to the files -> /Users/jiayikoh/Downloads/07 GI Commission 3/\\n\")\n",
    "\n",
    "while os.path.isdir(folder_path) != True:\n",
    "    folder_path = input(\"Directory does not exist, please re-enter the directory to the files -> /Users/jiayikoh/Downloads/07 GI Commission 3/\\n\")\n",
    "#os.chdir(folder_path)\n",
    "os.chdir('/Users/jiayikoh/KJY/IPP/GI_Commission_20240805')\n",
    "\n",
    "# state the Comm Mth.\n",
    "input_comm_mth = input(\"Please input the Comm Mth -> '2024-02-01'\\n\")\n",
    "comm_mth = datetime.datetime.strptime(input_comm_mth, \"%Y-%m-%d\").date()\n",
    "\n",
    "# Read all sheets into a dictionary of dataframes\n",
    "pattern='C1A'\n",
    "file_pattern = pattern + '*.xls*' \n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "uob_cb_dict = pd.read_excel(matching_files[0], sheet_name=None, header=4)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "uob_cb = pd.concat(uob_cb_dict.values(), ignore_index=True)\n",
    "\n",
    "# Read all sheets into a dictionary of dataframes\n",
    "pattern='C2A'\n",
    "file_pattern = pattern + '*.xls*' \n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "rhb_cb_dict = pd.read_excel(matching_files[0], sheet_name=None, header=4)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "rhb_cb = pd.concat(rhb_cb_dict.values(), ignore_index=True)\n",
    "\n",
    "\n",
    "# extract the insurer name\n",
    "uob_cb['Extracted_Insurer'] = uob_cb['Name'].str.extract('([^\\s]+)', expand=False).str.lower()\n",
    "rhb_cb['Extracted_Insurer'] = rhb_cb['Name'].str.extract('([^\\s]+)', expand=False).str.lower()\n",
    "\n",
    "# concatenate both cashbook df\n",
    "all_cb = pd.concat([uob_cb, rhb_cb], axis=0)\n",
    "\n",
    "# to load the current P0 working file to get the previous information for the missing TFAR in some of the insurer's files\n",
    "pattern='P0'\n",
    "file_pattern = pattern + '*.xls*' \n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "wk_dict = pd.read_excel(matching_files[0], sheet_name=None)\n",
    "\n",
    "# define the keys to be concatenate\n",
    "paid_keys = [key for key in wk_dict.keys() if key.startswith('PAID')]\n",
    "unpaid_keys = [key for key in wk_dict.keys() if key.startswith('UNPAID')]\n",
    "\n",
    "# concatenate both\n",
    "all_keys = paid_keys + unpaid_keys\n",
    "\n",
    "# concatenate the corresponding dataframe\n",
    "previous_wk = pd.concat([wk_dict[key] for key in all_keys]).drop_duplicates()\n",
    "\n",
    "from gi_test import GI_commission\n",
    "\n",
    "from Name_matching import Name_matching\n",
    "\n",
    "gi = GI_commission(folder_path=folder_path, all_cb=all_cb, previous_wk=previous_wk)\n",
    "\n",
    "# concatenate all the df\n",
    "wk_df_1 = pd.concat([gi.run_aig_1('01_01A'), #aig\n",
    "                     gi.run_aig_1('01_01B'),\n",
    "                     gi.run_aig_1('01_01C'),\n",
    "                     gi.run_aig_1('01_01D'),\n",
    "                     gi.run_aig_1('01_01E'),\n",
    "           gi.run_aia_1('03_01A'), #aia\n",
    "           gi.run_aia_1('03_01B'),\n",
    "           gi.run_aia_1('03_01C'),\n",
    "           gi.run_aia_1('03_01D'),\n",
    "           gi.run_aia_1('03_01E'),\n",
    "           gi.run_allianz_1('04_01A'), #allianz\n",
    "           gi.run_allianz_1('04_01B'),\n",
    "           gi.run_allianz_1('04_01C'),\n",
    "           gi.run_allianz_1('04_01D'),\n",
    "           gi.run_allianz_1('04_01E'),\n",
    "           gi.run_allied_1('06_01A'), #allied\n",
    "           gi.run_allied_1('06_01B'),\n",
    "           gi.run_allied_1('06_01C'),\n",
    "           gi.run_allied_1('06_01D'),\n",
    "           gi.run_allied_1('06_01E'),\n",
    "           gi.run_allied_2('06_02A'),\n",
    "           gi.run_allied_2('06_02B'),\n",
    "           gi.run_allied_2('06_02C'),\n",
    "           gi.run_allied_2('06_02D'),\n",
    "           gi.run_allied_2('06_02E'),\n",
    "           gi.run_chubb_1('09_01A'), #chubb\n",
    "           gi.run_chubb_1('09_01B'),\n",
    "           gi.run_chubb_1('09_01C'),\n",
    "           gi.run_chubb_1('09_01D'),\n",
    "           gi.run_chubb_1('09_01E'),\n",
    "           gi.run_chubb_2('09_02A'),\n",
    "           gi.run_chubb_2('09_02B'),\n",
    "           gi.run_chubb_2('09_02C'),\n",
    "           gi.run_chubb_2('09_02D'),\n",
    "           gi.run_chubb_2('09_02E'),\n",
    "           gi.run_chubb_3('09_03A'),\n",
    "           gi.run_chubb_3('09_03B'),\n",
    "           gi.run_chubb_3('09_03C'),\n",
    "           gi.run_chubb_3('09_03D'),\n",
    "           gi.run_chubb_3('09_03E'),\n",
    "           gi.run_delta_1('11_01A'), #delta\n",
    "           gi.run_delta_1('11_01B'),\n",
    "           gi.run_delta_1('11_01C'),\n",
    "           gi.run_delta_1('11_01D'),\n",
    "           gi.run_delta_1('11_01E'),\n",
    "           gi.run_ergo_1('13_01A'), #ergo\n",
    "           gi.run_ergo_1('13_01B'),\n",
    "           gi.run_ergo_1('13_01C'),\n",
    "           gi.run_ergo_1('13_01D'),\n",
    "           gi.run_ergo_1('13_01E'),\n",
    "           gi.run_fwd_1('15_01A'), #fwd\n",
    "           gi.run_fwd_1('15_01B'),\n",
    "           gi.run_fwd_1('15_01C'),\n",
    "           gi.run_fwd_1('15_01D'),\n",
    "           gi.run_fwd_1('15_01E'),\n",
    "           gi.run_hla_1('19_01A'), #hla\n",
    "           gi.run_hla_1('19_01B'),\n",
    "           gi.run_hla_1('19_01C'),\n",
    "           gi.run_hla_1('19_01D'),\n",
    "           gi.run_hla_1('19_01E'),\n",
    "           gi.run_hsbc_1('20_01A'), #hsbc\n",
    "           gi.run_hsbc_1('20_01B'),\n",
    "           gi.run_hsbc_1('20_01C'),\n",
    "           gi.run_hsbc_1('20_01D'),\n",
    "           gi.run_hsbc_1('20_01E'),\n",
    "           gi.run_hsbc_2('20_02A'),\n",
    "           gi.run_hsbc_2('20_02B'),\n",
    "           gi.run_hsbc_2('20_02C'),\n",
    "           gi.run_hsbc_2('20_02D'),\n",
    "           gi.run_hsbc_2('20_02E'),\n",
    "           gi.run_income_1('21_01A'), #income\n",
    "           gi.run_income_1('21_01B'),\n",
    "           gi.run_income_1('21_01C'),\n",
    "           gi.run_income_1('21_01D'),\n",
    "           gi.run_income_1('21_01E'),\n",
    "           gi.run_liberty_1('23_01A'), #liberty\n",
    "           gi.run_liberty_1('23_01B'),\n",
    "           gi.run_liberty_1('23_01C'),\n",
    "           gi.run_liberty_1('23_01D'),\n",
    "           gi.run_liberty_1('23_01E'),\n",
    "           gi.run_msig_1('25_01A'), #msig\n",
    "           gi.run_msig_1('25_01B'),\n",
    "           gi.run_msig_1('25_01C'),\n",
    "           gi.run_msig_1('25_01D'),\n",
    "           gi.run_msig_1('25_01E'),\n",
    "           gi.run_nhi_1('26_01A'), #nhi\n",
    "           gi.run_nhi_1('26_01B'),\n",
    "           gi.run_nhi_1('26_01C'),\n",
    "           gi.run_nhi_1('26_01D'),\n",
    "           gi.run_nhi_1('26_01E'),\n",
    "           gi.run_qbe_1('28_01A'), #qbe\n",
    "           gi.run_qbe_1('28_01B'),\n",
    "           gi.run_qbe_1('28_01C'),\n",
    "           gi.run_qbe_1('28_01D'),\n",
    "           gi.run_qbe_1('28_01E'),\n",
    "           gi.run_singlife_1('30_01A'), #singlife\n",
    "           gi.run_singlife_1('30_01B'),\n",
    "           gi.run_singlife_1('30_01C'),\n",
    "           gi.run_singlife_1('30_01D'),\n",
    "           gi.run_singlife_1('30_01E'),\n",
    "           gi.run_sompo_1('31_01A'), #sompo\n",
    "           gi.run_sompo_1('31_01B'),\n",
    "           gi.run_sompo_1('31_01C'),\n",
    "           gi.run_sompo_1('31_01D'),\n",
    "           gi.run_sompo_1('31_01E'),\n",
    "           gi.run_tm_1('32_01A'), #tm\n",
    "           gi.run_tm_1('32_01B'),\n",
    "           gi.run_tm_1('32_01C'),\n",
    "           gi.run_tm_1('32_01D'),\n",
    "           gi.run_tm_1('32_01E'),\n",
    "           gi.run_tm_2('33_01A'),\n",
    "           gi.run_tm_2('33_01B'),\n",
    "           gi.run_tm_2('33_01C'),\n",
    "           gi.run_tm_2('33_01D'),\n",
    "           gi.run_tm_2('33_01E')\n",
    "           ], ignore_index=True).drop_duplicates()\n",
    "\n",
    "\n",
    "name_far = Name_matching(folder_path=folder_path, all_advisor_df=wk_df_1)\n",
    "\n",
    "comm_far = name_far.matching_far()\n",
    "\n",
    "# get the match score of 100 only\n",
    "comm_far = comm_far[comm_far['matching_score']==100]\n",
    "\n",
    "# function to extract the first element after \"-\"\n",
    "\n",
    "pattern = r\"(?<=-).*.*(?=-)\"\n",
    "\n",
    "def extract_after_or_first(text):\n",
    "  \"\"\"Extracts the element after the first \"-\" or the first word.\n",
    "\n",
    "  Args:\n",
    "      text: The string to extract from.\n",
    "\n",
    "  Returns:\n",
    "      The first element after the first \"-\" or the first word, or None if empty.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "      return match.group()\n",
    "    else:\n",
    "      return None\n",
    "  except:\n",
    "    return None\n",
    "  \n",
    "# this is to extract the first element after \"-\" and strip off the whitespace at the start & back\n",
    "wk_df_1['TFAR'] = np.where(wk_df_1['TFAR'].apply(extract_after_or_first) == None, wk_df_1['TFAR'], wk_df_1['TFAR'].apply(extract_after_or_first).str.strip())\n",
    "\n",
    "# read the GI e-submission list\n",
    "file_pattern = 'W3*.xls*'\n",
    "\n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "# load the e-submission list for the referrer list\n",
    "e_sub = pd.read_excel(matching_files[0])\n",
    "\n",
    "# this is to get the latest TFAR & Referrer based on the latest policy no (we will have to match to find the TFAR)\n",
    "# group by the policy number and then filter the latest file name (which is a datetime)\n",
    "previous_wk_pol_1 = previous_wk.sort_values(by=['File Name'], ascending=False).groupby(by=['Policy No'])\n",
    "\n",
    "# to get the first one which is the latest information\n",
    "previous_wk_pol_2 = previous_wk_pol_1.head(1)\n",
    "\n",
    "# this is to get the latest TFAR & Referrer based on the latest Insured Name (we will have to match to find the TFAR)\n",
    "# group by the Insured Name and then filter the latest file name (which is a datetime)\n",
    "previous_wk_ins_1 = previous_wk.sort_values(by=['File Name'], ascending=False).groupby(by=['Insured '])\n",
    "\n",
    "# to get the first one which is the latest information\n",
    "previous_wk_ins_2 = previous_wk_ins_1.head(1)\n",
    "\n",
    "# 1st: merge with e_sub using policy number to get Referrer name and TFAR Name\n",
    "wk_df_1_merge_1 = pd.merge(wk_df_1, e_sub[['Policy No', 'Name of TFAR', 'Name of Referral']].rename(columns={'Name of TFAR':'TFAR_esub', 'Name of Referral':'Referral_esub'}), how='left', left_on='Policy no.', right_on='Policy No')\n",
    "\n",
    "# 2nd: merge with previous_wk using policy number to get Referral and TFAR\n",
    "wk_df_1_merge_2 = pd.merge(wk_df_1_merge_1, previous_wk_pol_2[['Policy No', 'TFAR', 'REFFERAL ', 'Insured ']].rename(columns={'TFAR':'TFAR_wk_pol', 'REFFERAL ':'Referral_wk_pol', 'Insured ':'Insured_wk_pol'}), how='left', left_on='Policy no.', right_on='Policy No')\n",
    "\n",
    "# 3rd: merge with previous_wk using insured name to get Referral and TFAR\n",
    "wk_df_1_merge_3 = pd.merge(wk_df_1_merge_2, previous_wk_ins_2[['Insured ', 'TFAR', 'REFFERAL ']].rename(columns={'TFAR':'TFAR_wk_ins', 'REFFERAL ':'Referral_wk_ins'}), how='left', left_on=wk_df_1_merge_2['Insured Name'].str.lower(), right_on=previous_wk_ins_2['Insured '].str.lower())\n",
    "\n",
    "df_filled_1 = wk_df_1_merge_3[['TFAR', 'TFAR_wk_pol', 'TFAR_wk_ins', 'TFAR_esub']].rename(columns={'TFAR':'CommState_TFAR', 'TFAR_wk_pol':'PreviousWork_TFAR_pol', \n",
    "                                                                                                   'TFAR_wk_ins':'PreviousWork_TFAR_ins', 'TFAR_esub':'esub_TFAR'})\n",
    "\n",
    "# fill missing values with column-wise forward fill\n",
    "df_filled_2 = df_filled_1.fillna(axis=1, method='ffill')\n",
    "\n",
    "def get_all_values_and_first_available(row):\n",
    "  \"\"\"\n",
    "  Safely retrieves the first available value in a row, handling cases with all NaN values.\n",
    "\n",
    "  Args:\n",
    "      row: A pandas Series representing a row from the DataFrame.\n",
    "\n",
    "  Returns:\n",
    "      The first non-NaN value in the row, or None if all values are NaN.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # get all values including NaN using tolist()\n",
    "    all_values = row.tolist()\n",
    "    \n",
    "    # get the first available value\n",
    "    first_available = row.dropna().iloc[0]\n",
    "    \n",
    "    # check if any value is nan\n",
    "    has_nan = row.isnull().any()\n",
    "    # Attempt to get the first non-NaN value using iloc[0]\n",
    "    return all_values, first_available, has_nan\n",
    "  except IndexError:\n",
    "    # Handle cases where all values are NaN (IndexError due to empty Series)\n",
    "    return None\n",
    "  \n",
    "def extract_nth_element(data, nth):\n",
    "  \"\"\"Extracts the second element from the tuple, handling None values.\n",
    "\n",
    "  Args:\n",
    "      data: The tuple (element from the Series).\n",
    "\n",
    "  Returns:\n",
    "      The second element from the tuple, or None if the data is None or IndexError occurs.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    return data[nth]\n",
    "  except (IndexError, TypeError):\n",
    "    return None\n",
    "  \n",
    "# extract all TFAR value from different source\n",
    "df_filled_2['all_TFARs'] = df_filled_2.apply(get_all_values_and_first_available, axis=1).apply(lambda x: extract_nth_element(x, 0))\n",
    "\n",
    "# take the first available value\n",
    "# df_filled_2['first_available'] = df_filled_2[['TFAR_wk_pol', 'TFAR_wk_ins', 'TFAR_esub']].apply(get_all_values_and_first_available, axis=1).apply(lambda x: extract_nth_element(x, 1))\n",
    "df_filled_2['first_available'] = df_filled_2.apply(get_all_values_and_first_available, axis=1).apply(lambda x: extract_nth_element(x, 1))\n",
    "\n",
    "# concatenate the df_filled_2 with the main df\n",
    "wk_df_1_merge_4 = pd.concat([wk_df_1_merge_3, df_filled_2], axis=1)\n",
    "\n",
    "# rule: if TFAR is nan, take the first available\n",
    "wk_df_1_merge_4['Final_TFAR'] = np.where(wk_df_1_merge_4['TFAR'].isna(), wk_df_1_merge_4['first_available'], wk_df_1_merge_4['TFAR'])\n",
    "\n",
    "# rule: if insured is nan, take from the previous working file\n",
    "wk_df_1_merge_4['Final_Insured'] = np.where(wk_df_1_merge_4['Insured Name'].isna(), wk_df_1_merge_4['Insured_wk_pol'], wk_df_1_merge_4['Insured Name'])\n",
    "\n",
    "wk_df_1 = wk_df_1_merge_4.copy()\n",
    "\n",
    "# output another data featuring the TFAR from different source\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(f'{folder_path}/Working_file_check_tfar_{date.today():%Y%m%d}.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "wk_df_1_merge_4[['Policy no.', 'CommState_TFAR', 'PreviousWork_TFAR_pol', 'PreviousWork_TFAR_ins', 'esub_TFAR', 'first_available']].drop_duplicates().to_excel(writer)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "wk_df_1['TFAR'] = wk_df_1['Final_TFAR']\n",
    "wk_df_1['Insured Name'] = wk_df_1['Final_Insured']\n",
    "\n",
    "# read the FAR masterlist\n",
    "file_pattern = 'W1*.xls*' \n",
    "\n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "far_master = pd.read_excel(matching_files[0], skiprows=1)\n",
    "\n",
    "\n",
    "# read the GI e-submission list\n",
    "file_pattern = 'W3*.xls*'\n",
    "\n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "# load the e-submission list for the referrer list\n",
    "e_sub = pd.read_excel(matching_files[0])\n",
    "\n",
    "# merge with itself to get the manager's advisory group\n",
    "far_master_1 = pd.merge(far_master, far_master[['NEW FAR NAME', 'Advisory Group']].rename(columns={'NEW FAR NAME':'Manager Name', 'Advisory Group':'Manager AG'}), \n",
    "                      how='left', left_on='Manager', right_on='Manager Name').drop_duplicates()\n",
    "\n",
    "# merge with itself to get the old agent's manager details\n",
    "far_master_2 = pd.merge(far_master_1, far_master[['NEW FAR NAME', 'Manager', 'Advisory Group', 'Mgr Rate']].rename(columns={'NEW FAR NAME':'Old_FAR', 'Manager':'Manager 2 Name', 'Advisory Group':'Manager 2 AG', 'Mgr Rate':'Manager 2 GR%'}),\n",
    "                     how='left', left_on='FAR (Old)', right_on='Old_FAR').rename(columns={'Mgr Rate':'Manager GR%'}).drop_duplicates()\n",
    "\n",
    "\n",
    "# will have to remove the \"all_TFARs\" column as it contains lists -> will not be able to do .drop_duplicates()\n",
    "del wk_df_1['all_TFARs']\n",
    "del wk_df_1['Policy No_x']\n",
    "\n",
    "# find the name in FAR masterlist using the revised name list provided by the name matching algorithm\n",
    "wk_df_2 = pd.merge(wk_df_1, comm_far[['ADVISER', 'matched_name']], how='left', left_on='TFAR', right_on='ADVISER')\n",
    "\n",
    "# match the adviser details using the name matching list\n",
    "wk_df_3 = pd.merge(wk_df_2, far_master_2[['NEW FAR NAME', 'Advisory Group', 2024, 'FAR Status', 'NRIC', 'Manager Name', 'Manager AG', 'Manager GR%', 'Manager 2 Name', 'Manager 2 AG', 'Manager 2 GR%']].rename(columns={2024:'GR%', 'NRIC':'FAR_NRIC'}), how='left', left_on='matched_name', right_on='NEW FAR NAME')\n",
    "\n",
    "# match policy number to get referrer details\n",
    "wk_df_4 = pd.merge(wk_df_3, e_sub[['Policy No', 'Name of Referral']].rename(columns={'Name of Referral':'Referrer'}), how='left', left_on='Policy no.', right_on='Policy No').drop_duplicates()\n",
    "\n",
    "# remove Not Applicable in Referrer\n",
    "wk_df_4['Referrer'] = np.where(wk_df_4['Referrer'] == 'Not Applicable', np.NaN, wk_df_4['Referrer'])\n",
    "\n",
    "# find the name in FAR masterlist using the revised name list provided by the name matching algorithm\n",
    "wk_df_5 = pd.merge(wk_df_4, comm_far[['ADVISER', 'matched_name']].rename(columns={'matched_name':'referrer_matched_name'}), how='left', left_on='Referrer', right_on='ADVISER')\n",
    "\n",
    "\n",
    "# label GST Type\n",
    "insurer_list = list(wk_df_5['Insurer'].unique())\n",
    "\n",
    "new_df = []\n",
    "\n",
    "for i, j in enumerate(insurer_list):\n",
    "    df_working = wk_df_5[wk_df_5['Insurer'] == j]\n",
    "    if j == 'AIG-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'V')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'AIA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'ALLIANZ-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'ALLIED-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'CHUBB-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'DELTA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'FWD-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['$txn gst commission'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)   \n",
    "    elif j == 'HLA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'GE-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'HSBC LIFE-GI':\n",
    "        df_working['GST Type'] = 'V'\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'INCOME-GI':\n",
    "        df_working['GST Type'] = 'Z'\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'INDIA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'LIBERTY-GI':\n",
    "        df_working['GST Type'] = 'V'\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'MSIG-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'NHI-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'QBE-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'SINGLIFE-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'SOMPO-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'V')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'TM-GI':\n",
    "        df_working['GST Type'] = 'G'\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'TM LIFE-GI':\n",
    "        df_working['GST Type'] = 'G'\n",
    "        new_df.append(df_working)\n",
    "        \n",
    "        \n",
    "wk_df_6 = pd.concat(new_df)\n",
    "    \n",
    "\n",
    "# make a soft copy\n",
    "wk_df = wk_df_6.drop_duplicates().copy()\n",
    "\n",
    "# delete TFAR column\n",
    "del wk_df['TFAR']\n",
    "del wk_df['Referrer']\n",
    "\n",
    "# insert a column for Comm Mth.\n",
    "wk_df['Comm Mth.'] = comm_mth\n",
    "\n",
    "# get the required fields\n",
    "columns = ['Comm Mth.', 'Cashbook ref. no.', 'Policy no.', 'Insured Name', 'Insurer', 'TFAR', 'Referrer', 'Advisory Group', 'Comm.Recd (with GST)', 'GST Type', 'GR%', 'Pol Date', 'FAR Receiving Comm Status', 'FAR_NRIC', 'Manager Name', 'Manager AG', 'Manager GR%', 'Manager 2 Name', 'Manager 2 AG', 'Manager 2 GR%']\n",
    "\n",
    "wk_df = wk_df.rename(columns={'Advisory Group_y':'Advisory Group', 'matched_name':'TFAR', 'referrer_matched_name':'Referrer', 'GR%_y':'GR%', 'FAR Status':'FAR Receiving Comm Status'})[columns].drop_duplicates()\n",
    "\n",
    "# below function extract the first two digits after the alphabets for the cashbook year\n",
    "def extract_first_two_digits(text):\n",
    "  if pd.isna(text):\n",
    "    return np.nan  # Return NaN if the value is NaN\n",
    "  else:\n",
    "    match = re.search(r'\\d{2}', text)\n",
    "    return match.group() if match else np.nan  # Return NaN if no match found\n",
    "\n",
    "wk_df['Cashbook ref. no.'] = wk_df['Cashbook ref. no.'] + ' (Yr 20' + wk_df['Cashbook ref. no.'].apply(extract_first_two_digits) + ')'\n",
    "\n",
    "# title case for Insured Name and Advisory Group\n",
    "#wk_df['Insured Name'] = wk_df['Insured Name'].str.title()\n",
    "#wk_df['Advisory Group'] = wk_df['Advisory Group'].str.title()\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(f'{folder_path}/S1 - Working_file_{date.today():%Y%m%d}.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "wk_df.to_excel(writer, sheet_name='Data')\n",
    "#df_1_pivot.to_excel(writer, sheet_name='Pivot')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
