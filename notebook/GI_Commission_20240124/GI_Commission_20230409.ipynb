{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "# standard import\n",
    "from datetime import date\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "os.chdir('/Users/jiayikoh/KJY/IPP')\n",
    "\n",
    "# state the folder_path\n",
    "folder_path = '/Users/jiayikoh/Downloads/Dec 23 (Payout in Jan 2024)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all sheets into a dictionary of dataframes\n",
    "uob_cb_dict = pd.read_excel('/Users/jiayikoh/Downloads/Dec 23 (Payout in Jan 2024)/IPP-FA-UOB SGD Yr 2023.xlsx', sheet_name=None, header=4)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "uob_cb = pd.concat(uob_cb_dict.values(), ignore_index=True)\n",
    "\n",
    "\n",
    "# Read all sheets into a dictionary of dataframes\n",
    "rhb_cb_dict = pd.read_excel('/Users/jiayikoh/Downloads/Dec 23 (Payout in Jan 2024)/IPP-FA-RHB SGD Yr 2023.xlsx', sheet_name=None, header=4)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "rhb_cb = pd.concat(rhb_cb_dict.values(), ignore_index=True)\n",
    "\n",
    "\n",
    "# extract the insurer name\n",
    "uob_cb['Extracted_Insurer'] = uob_cb['Name'].str.extract('([^\\s]+)', expand=False).str.lower()\n",
    "rhb_cb['Extracted_Insurer'] = rhb_cb['Name'].str.extract('([^\\s]+)', expand=False).str.lower()\n",
    "\n",
    "# concatenate both cashbook df\n",
    "all_cb = pd.concat([uob_cb, rhb_cb], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the current P0 working file to get the previous information for the missing TFAR in some of the insurer's files\n",
    "wk_dict = pd.read_excel('/Users/jiayikoh/Downloads/Dec 23 (Payout in Jan 2024)/P0.12 Dec 23 (Payout in Jan 2024).xlsx', sheet_name=None)\n",
    "\n",
    "# define the keys to be concatenate\n",
    "paid_keys = [key for key in wk_dict.keys() if key.startswith('PAID')]\n",
    "unpaid_keys = [key for key in wk_dict.keys() if key.startswith('UNPAID')]\n",
    "\n",
    "# concatenate both\n",
    "all_keys = paid_keys + unpaid_keys\n",
    "\n",
    "# concatenate the corresponding dataframe\n",
    "previous_wk = pd.concat([wk_dict[key] for key in all_keys]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gi_test import GI_commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Name_matching import Name_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi = GI_commission(folder_path=folder_path, all_cb=all_cb, previous_wk=previous_wk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py:1204: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "# concatenate all the df\n",
    "wk_df_1 = pd.concat([gi.run_aig_1('01_01A'),\n",
    "           gi.run_aig_1('01_01B'),\n",
    "           gi.run_aia_1('03_01A'),\n",
    "           gi.run_allianz_1('04_01A'),\n",
    "           gi.run_allied_1('06_01A'),\n",
    "           gi.run_allied_1('06_01B'),\n",
    "           gi.run_allied_2('06_02A'),\n",
    "           gi.run_chubb_1('09_01A'),\n",
    "           gi.run_chubb_1('09_01B'),\n",
    "           gi.run_ergo_1('13_01A'),\n",
    "           gi.run_fwd_1('15_01A'),\n",
    "           gi.run_ge_life_1('18_01A'),\n",
    "           gi.run_hla_1('19_01A'),\n",
    "           gi.run_hsbc_1('20_01A'),\n",
    "           gi.run_income_1('21_01A'),\n",
    "           gi.run_liberty_1('23_01A'),\n",
    "           gi.run_liberty_1('23_01B'),\n",
    "           gi.run_liberty_1('23_01C'),\n",
    "           gi.run_liberty_1('23_01D'),\n",
    "           gi.run_nhi_1('26_01A'),\n",
    "           gi.run_qbe_1('28_01A'),\n",
    "           gi.run_singlife_1('30_01A'),\n",
    "           gi.run_sompo_1('31_01A'),\n",
    "           gi.run_sompo_1('31_01B')\n",
    "           ], ignore_index=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_far = Name_matching(folder_path=folder_path, all_advisor_df=wk_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_far = name_far.matching_far()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the FAR masterlist\n",
    "file_pattern = 'W1*.xls*' \n",
    "\n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "far_master = pd.read_excel(matching_files[0], skiprows=1)\n",
    "\n",
    "# merge with itself to get the old agent's manager details\n",
    "far_master = pd.merge(far_master, far_master[['NEW FAR NAME', 'Manager', 'Mgr Rate']].rename(columns={'Manager':'Manager 2 Name', 'Mgr Rate':'Manager 2 GR%'}),\n",
    "                     how='left', left_on='FAR (Old)', right_on='NEW FAR NAME').rename(columns={'NEW FAR NAME_x':'NEW FAR NAME'})\n",
    "\n",
    "# find the name in FAR masterlist using the revised name list provided by the name matching algorithm\n",
    "wk_df_2 = pd.merge(wk_df_1, comm_far[['ADVISER', 'matched_name']], how='left', left_on='TFAR', right_on='ADVISER')\n",
    "\n",
    "# match the adviser details using the name matching list\n",
    "wk_df_3 = pd.merge(wk_df_2, far_master[['NEW FAR NAME', 'Advisory Group', 2023, 'FAR Status']].rename(columns={2023:'GR%'}), how='left', left_on='matched_name', right_on='NEW FAR NAME')\n",
    "\n",
    "# match policy number to get referrer details\n",
    "wk_df_4 = pd.merge(wk_df_3, e_sub[['Policy No', 'Name of Referral']].rename(columns={'Name of Referral':'Referrer'}), how='left', left_on='Policy no.', right_on='Policy No').drop_duplicates()\n",
    "\n",
    "# remove Not Applicable in Referrer\n",
    "wk_df_4['Referrer'] = np.where(wk_df_4['Referrer'] == 'Not Applicable', np.NaN, wk_df_4['Referrer'])\n",
    "\n",
    "# find the name in FAR masterlist using the revised name list provided by the name matching algorithm\n",
    "wk_df_5 = pd.merge(wk_df_4, comm_far[['ADVISER', 'matched_name']].rename(columns={'matched_name':'referrer_matched_name'}), how='left', left_on='Referrer', right_on='ADVISER')\n",
    "\n",
    "\n",
    "# label GST Type\n",
    "insurer_list = list(wk_df_5['Insurer'].unique())\n",
    "\n",
    "new_df = []\n",
    "\n",
    "for i, j in enumerate(insurer_list):\n",
    "    df_working = wk_df_5[wk_df_5['Insurer'] == j]\n",
    "    if j == 'AIG-GI':\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'AIA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['Sum of GST'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'ALLIED WORLD-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'ALLIANZ-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST on Commission'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'CHUBB-GI':\n",
    "        new_df.append(df_working)\n",
    "        #df_working['GST Type'] = np.where(df_working['Sum of GST'] == 0, 'Z', 'G')\n",
    "    elif j == 'FWD-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['$txn gst commission'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'GE-GI':\n",
    "        new_df.append(df_working)\n",
    "        #df_working['GST Type'] = np.where(df_working['$txn gst commission'] == 0, 'Z', 'G')\n",
    "    elif j == 'HSBC-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'INCOME-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['$txn gst commission'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'INDIA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'LIBERTY-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'MSIG-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'QBE-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'SINGLIFE-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'SOMPO-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'V')\n",
    "        new_df.append(df_working)\n",
    "        \n",
    "        \n",
    "wk_df_6 = pd.concat(new_df)\n",
    "    \n",
    "\n",
    "# make a soft copy\n",
    "wk_df = wk_df_6.drop_duplicates().copy()\n",
    "\n",
    "# delete TFAR column\n",
    "del wk_df['TFAR']\n",
    "del wk_df['Referrer']\n",
    "\n",
    "# get the required fields\n",
    "columns = ['Cashbook ref. no.', 'Policy no.', 'Insured Name', 'Insurer', 'TFAR', 'Referrer', 'Advisory Group', 'Comm.Recd (with GST)', 'GST Type', 'GR%', 'Pol Date', 'JY_comment', 'FAR Receiving Comm Status']\n",
    "\n",
    "wk_df = wk_df.rename(columns={'Advisory Group_y':'Advisory Group', 'matched_name':'TFAR', 'referrer_matched_name':'Referrer', 'GR%_y':'GR%', 'FAR Status':'FAR Receiving Comm Status'})[columns].drop_duplicates()\n",
    "\n",
    "# title case for Insured Name and Advisory Group\n",
    "#wk_df['Insured Name'] = wk_df['Insured Name'].str.title()\n",
    "#wk_df['Advisory Group'] = wk_df['Advisory Group'].str.title()\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(f'{folder_path}Working_file_{date.today():%Y%m%d}.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "wk_df.to_excel(writer, sheet_name='Data')\n",
    "#df_1_pivot.to_excel(writer, sheet_name='Pivot')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is the final script for GI Commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard import\n",
    "from datetime import date\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "folder_path = input(\"Please input the directory to the files -> /Users/jiayikoh/Downloads/07 GI Commission 3/\\n\")\n",
    "\n",
    "while os.path.isdir(folder_path) != True:\n",
    "    folder_path = input(\"Directory does not exist, please re-enter the directory to the files -> /Users/jiayikoh/Downloads/07 GI Commission 3/\\n\")\n",
    "os.chdir(folder_path)\n",
    "\n",
    "uob_cashbook = input(\"UOB cashbook file name\\n\")\n",
    "\n",
    "rhb_cashbook = input(\"RHB cashbook file name\\n\")\n",
    "\n",
    "# load the class for all insurer functions\n",
    "from gi_test import GI_commission\n",
    "\n",
    "# load the class for name matching function\n",
    "from Name_matching import Name_matching\n",
    "\n",
    "\n",
    "# Read all sheets into a dictionary of dataframes\n",
    "uob_cb_dict = pd.read_excel(uob_cashbook, sheet_name=None, header=4)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "uob_cb = pd.concat(uob_cb_dict.values(), ignore_index=True)\n",
    "\n",
    "# Read all sheets into a dictionary of dataframes\n",
    "rhb_cb_dict = pd.read_excel(rhb_cashbook, sheet_name=None, header=4)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "rhb_cb = pd.concat(rhb_cb_dict.values(), ignore_index=True)\n",
    "\n",
    "\n",
    "# extract the insurer name\n",
    "uob_cb['Extracted_Insurer'] = uob_cb['Name'].str.extract('([^\\s]+)', expand=False).str.lower()\n",
    "rhb_cb['Extracted_Insurer'] = rhb_cb['Name'].str.extract('([^\\s]+)', expand=False).str.lower()\n",
    "\n",
    "# concatenate both cashbook df\n",
    "all_cb = pd.concat([uob_cb, rhb_cb], axis=0)\n",
    "\n",
    "\n",
    "# to load the current P0 working file to get the previous information for the missing TFAR in some of the insurer's files\n",
    "wk_dict = pd.read_excel('/Users/jiayikoh/Downloads/Dec 23 (Payout in Jan 2024)/P0.12 Dec 23 (Payout in Jan 2024).xlsx', sheet_name=None)\n",
    "\n",
    "# define the keys to be concatenate\n",
    "paid_keys = [key for key in wk_dict.keys() if key.startswith('PAID')]\n",
    "unpaid_keys = [key for key in wk_dict.keys() if key.startswith('UNPAID')]\n",
    "\n",
    "# concatenate both\n",
    "all_keys = paid_keys + unpaid_keys\n",
    "\n",
    "# concatenate the corresponding dataframe\n",
    "previous_wk = pd.concat([wk_dict[key] for key in all_keys]).drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# instantiate the GI Commission class\n",
    "gi = GI_commission(folder_path=folder_path, all_cb=all_cb, previous_wk=previous_wk)\n",
    "\n",
    "\n",
    "# concatenate all the df\n",
    "wk_df_1 = pd.concat([gi.run_aig_1('01_01A'),\n",
    "           gi.run_aig_1('01_01B'),\n",
    "           gi.run_aia_1('03_01A'),\n",
    "           gi.run_allianz_1('04_01A'),\n",
    "           gi.run_allied_1('06_01A'),\n",
    "           gi.run_allied_1('06_01B'),\n",
    "           gi.run_allied_2('06_02A'),\n",
    "           gi.run_chubb_1('09_01A'),\n",
    "           gi.run_chubb_1('09_01B'),\n",
    "           gi.run_ergo_1('13_01A'),\n",
    "           gi.run_fwd_1('15_01A'),\n",
    "           gi.run_ge_life_1('18_01A'),\n",
    "           gi.run_hla_1('19_01A'),\n",
    "           gi.run_hsbc_1('20_01A'),\n",
    "           gi.run_income_1('21_01A'),\n",
    "           gi.run_liberty_1('23_01A'),\n",
    "           gi.run_liberty_1('23_01B'),\n",
    "           gi.run_liberty_1('23_01C'),\n",
    "           gi.run_liberty_1('23_01D'),\n",
    "           gi.run_nhi_1('26_01A'),\n",
    "           gi.run_qbe_1('28_01A'),\n",
    "           gi.run_singlife_1('30_01A'),\n",
    "           gi.run_sompo_1('31_01A'),\n",
    "           gi.run_sompo_1('31_01B')\n",
    "           ], ignore_index=True).drop_duplicates()\n",
    "\n",
    "\n",
    "# instantiate the name matching class\n",
    "name_far = Name_matching(folder_path=folder_path, all_advisor_df=wk_df_1)\n",
    "\n",
    "# get the matched FAR name\n",
    "comm_far = name_far.matching_far()\n",
    "\n",
    "\n",
    "# read the FAR masterlist\n",
    "file_pattern = 'W1*.xls*' \n",
    "\n",
    "matching_files = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "far_master = pd.read_excel(matching_files[0], skiprows=1)\n",
    "\n",
    "# merge with itself to get the old agent's manager details\n",
    "far_master = pd.merge(far_master, far_master[['NEW FAR NAME', 'Manager', 'Mgr Rate']].rename(columns={'Manager':'Manager 2 Name', 'Mgr Rate':'Manager 2 GR%'}),\n",
    "                     how='left', left_on='FAR (Old)', right_on='NEW FAR NAME').rename(columns={'NEW FAR NAME_x':'NEW FAR NAME'})\n",
    "\n",
    "# find the name in FAR masterlist using the revised name list provided by the name matching algorithm\n",
    "wk_df_2 = pd.merge(wk_df_1, comm_far[['ADVISER', 'matched_name']], how='left', left_on='TFAR', right_on='ADVISER')\n",
    "\n",
    "# match the adviser details using the name matching list\n",
    "wk_df_3 = pd.merge(wk_df_2, far_master[['NEW FAR NAME', 'Advisory Group', 2023, 'FAR Status']].rename(columns={2023:'GR%'}), how='left', left_on='matched_name', right_on='NEW FAR NAME')\n",
    "\n",
    "# match policy number to get referrer details\n",
    "wk_df_4 = pd.merge(wk_df_3, e_sub[['Policy No', 'Name of Referral']].rename(columns={'Name of Referral':'Referrer'}), how='left', left_on='Policy no.', right_on='Policy No').drop_duplicates()\n",
    "\n",
    "# remove Not Applicable in Referrer\n",
    "wk_df_4['Referrer'] = np.where(wk_df_4['Referrer'] == 'Not Applicable', np.NaN, wk_df_4['Referrer'])\n",
    "\n",
    "# find the name in FAR masterlist using the revised name list provided by the name matching algorithm\n",
    "wk_df_5 = pd.merge(wk_df_4, comm_far[['ADVISER', 'matched_name']].rename(columns={'matched_name':'referrer_matched_name'}), how='left', left_on='Referrer', right_on='ADVISER')\n",
    "\n",
    "\n",
    "# label GST Type\n",
    "insurer_list = list(wk_df_5['Insurer'].unique())\n",
    "\n",
    "new_df = []\n",
    "\n",
    "for i, j in enumerate(insurer_list):\n",
    "    df_working = wk_df_5[wk_df_5['Insurer'] == j]\n",
    "    if j == 'AIG-GI':\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'AIA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['Sum of GST'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'ALLIED WORLD-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'ALLIANZ-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST on Commission'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'CHUBB-GI':\n",
    "        new_df.append(df_working)\n",
    "        #df_working['GST Type'] = np.where(df_working['Sum of GST'] == 0, 'Z', 'G')\n",
    "    elif j == 'FWD-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['$txn gst commission'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'GE-GI':\n",
    "        new_df.append(df_working)\n",
    "        #df_working['GST Type'] = np.where(df_working['$txn gst commission'] == 0, 'Z', 'G')\n",
    "    elif j == 'HSBC-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'INCOME-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['$txn gst commission'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'INDIA-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'LIBERTY-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'MSIG-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'QBE-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'SINGLIFE-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'G')\n",
    "        new_df.append(df_working)\n",
    "    elif j == 'SOMPO-GI':\n",
    "        df_working['GST Type'] = np.where(df_working['GST amt'] == 0, 'Z', 'V')\n",
    "        new_df.append(df_working)\n",
    "        \n",
    "        \n",
    "wk_df_6 = pd.concat(new_df)\n",
    "    \n",
    "\n",
    "# make a soft copy\n",
    "wk_df = wk_df_6.drop_duplicates().copy()\n",
    "\n",
    "# delete TFAR column\n",
    "del wk_df['TFAR']\n",
    "del wk_df['Referrer']\n",
    "\n",
    "# get the required fields\n",
    "columns = ['Cashbook ref. no.', 'Policy no.', 'Insured Name', 'Insurer', 'TFAR', 'Referrer', 'Advisory Group', 'Comm.Recd (with GST)', 'GST Type', 'GR%', 'Pol Date', 'JY_comment', 'FAR Receiving Comm Status']\n",
    "\n",
    "wk_df = wk_df.rename(columns={'Advisory Group_y':'Advisory Group', 'matched_name':'TFAR', 'referrer_matched_name':'Referrer', 'GR%_y':'GR%', 'FAR Status':'FAR Receiving Comm Status'})[columns].drop_duplicates()\n",
    "\n",
    "# title case for Insured Name and Advisory Group\n",
    "#wk_df['Insured Name'] = wk_df['Insured Name'].str.title()\n",
    "#wk_df['Advisory Group'] = wk_df['Advisory Group'].str.title()\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(f'{folder_path}Working_file_{date.today():%Y%m%d}.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "wk_df.to_excel(writer, sheet_name='Data')\n",
    "#df_1_pivot.to_excel(writer, sheet_name='Pivot')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
